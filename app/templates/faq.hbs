<main>
  <h1>Frequently Asked Questions</h1>
  {{link-to '‚Üê Back to home' 'index'}}

  <h2 id='tokens'>Why do you use Personal Access Tokens?</h2>
  <p>
    GitHub created Personal Access Tokens for quick and dirty use. If you write
    a personal script that interacts with GitHub for you, you can paste the
    access token directly into your script so it's always authenticated.
  </p>
  <p>
    This is a security hole; it's an easy way to leak all your permissions,
    fast. To mitigate this, you can limit the permissions that each token has.
    Reciprocity only needs read-only access (the default).
  </p>
  <p>
    There is an alternative: the
    <a href='https://developer.github.com/v3/oauth/#web-application-flow'>web
    application flow</a>, where a web app will redirect you to GitHub, you
    authenticate, and GitHub gives the website a token to use. This is a
    smoother UX, but I didn't implement it because (1) it's a lot of work for a
    simple site like this, and (2) it requires that the website have a secret
    key (registered with GitHub), and since I'm hosting publicly on GitHub
    pages, I can't create a secret key.
  </p>

  <h2 id='localStorage'>Why store so much in localStorage?</h2>
  <p>
    This site saves a lot of data to localStorage, including your GitHub
    username, token, repositories you selected, and metric settings.
  </p>
  <p>
    It does this so it's easy for you come directly back to the results page,
    so you can see if your code review habits have improved.
  </p>
  <p>
    There is a security risk here: though this app is open-source, I could
    write another app under the domain azirbel.github.io, redirect you there,
    and have that app steal your localStorage data. If you provided a token,
    this would give me read-access to your GitHub.
  </p>

  <h2 id='starred'>Why use starred repositories?</h2>
  <p>
    Reciprocity lets you select repositories to analyze from a list of your
    starred repositories.
  </p>
  <p>
    The reason we use starred repositories is a technical limitation: the
    GitHub API doesn't a provide a way to get a list of every repository you've
    ever committed to or commented on.
  </p>
  <p>
    You might as well star repositories you care about, anyway!
  </p>

  <h2 id='reviewers'>How do you count reviewers?</h2>
  <p>
    We need some way to tell who reviewed a given pull request.
  </p>
  <p>
    My team uses a code word - LGTM (looks good to me) - to formally approve a
    pull request. We don't merge unless someone has given their LGTM. This
    makes it very easy for Reciprocity to search through comments looking for
    the phrase "LGTM".
  </p>
  <p>
    Of course, this approach can fail. But if we're lucky, mistakes will
    cancel out.
  </p>
  <p>
    One special case that Reciprocity does handle: if the author of a pull
    request comments on their own request, with "LGTM" and another author's
    name, then that other author will be counted as the reviewer for that pull
    request. This addresses a common pattern: the author commenting "verbal
    LGTM by @otherperson".
  </p>
  <p>
    If you use a phrase other than LGTM... I'm afraid Reciprocity don't support
    that right now. You can open an issue in my GitHub repo, or just fork this
    repository and add your own logic.
  </p>

  <h2 id='goodwill'>What is "goodwill"?</h2>
  <p>
    Goodwill is a measure of your "giving back" to the team by reviewing their
    code.
  </p>
  <p>
    The number itself will vary based on what metric you are using to analyze
    your reviews, but the idea is that positive goodwill means you've given
    more than you've gotten, and negative goodwill means you've gotten too much.
  </p>

  <h2 id='metrics'>How much weight do you give pull requests?</h2>
  <p>
    To be useful, Reciprocity needs a way to estimate how much work it took you
    (or someone else) to review a pull request.
  </p>
  <p>
    The most common metric is "lines of code". That's an option with
    Reciprocity, but it's a bad one. If you've reviewed even one huge refactor,
    suddenly your analysis is skewed by ten thousand lines of code.
  </p>
  <p>
    Below are the available metrics and what they mean.
  </p>
  <div class='wrapper'>
    <div class='left-1'><p>Complexity Score</p></div>
    <div class='right-2'><p>
      An estimate of how much time it took to review the pull request, on a
      scale of 1 to 12. We first score the lines of code on a scale of 1-4 (for
      tiny, small, medium, and large changes). Then we score the number of
      comments on a scale of 1-3 (to see how much discussion/debate/fixes were
      required). We multiply the two numbers together to give the most weight
      to large changes which generated lots of discussion.
    </p></div>
  </div>
  <div class='wrapper'>
    <div class='left-1'><p>Number</p></div>
    <div class='right-2'><p>
      A simple count of reviews: you get a point when you review someone else's
      code, and you lose a point when someone reviews your code. For extra
      detail, there are separate counts for large and small pull requests (more
      and less than 200 lines of code, respectively).
    </p></div>
  </div>
  <div class='wrapper'>
    <div class='left-1'><p>Lines of Code</p></div>
    <div class='right-2'><p>
      A measure of the lines of code touched in a pull request, calculated as
      addtions + deletions.
    </p></div>
  </div>

  <div class='button-block'>
    {{link-to 'Home' 'index' class='home-btn'}}
  </div>
</main>
